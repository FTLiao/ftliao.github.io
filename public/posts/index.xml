<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Feng-Ting Liao | Particles of Worlds</title>
    <link>http://localhost:3131/posts/</link>
    <description>Recent content in Posts on Feng-Ting Liao | Particles of Worlds</description>
    <image>
      <title>Feng-Ting Liao | Particles of Worlds</title>
      <url>http://localhost:3131/images/openimage.jpg</url>
      <link>http://localhost:3131/images/openimage.jpg</link>
    </image>
    <generator>Hugo -- 0.128.2</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 21 Aug 2024 13:05:44 +0200</lastBuildDate>
    <atom:link href="http://localhost:3131/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Learing Assistant for Education</title>
      <link>http://localhost:3131/posts/240821_learning_assistant_for_education/learning_assistant/</link>
      <pubDate>Wed, 21 Aug 2024 13:05:44 +0200</pubDate>
      <guid>http://localhost:3131/posts/240821_learning_assistant_for_education/learning_assistant/</guid>
      <description>There has been much development of ML in EduTech. One notable effort as an example is Andrej Karpathy&amp;rsquo;s effort in building a startup aiming to apply an AI learning assistant to education (link to news).
In this post, I will share my notes on such a learning assistant.
I will begin by defining my understanding of learning, followed by outline the properties of a learning assistant that effectively supports a learner&amp;rsquo;s educational journey.</description>
    </item>
    <item>
      <title>Learning at Test Time</title>
      <link>http://localhost:3131/posts/240807_learning_at_test_time/learning_at_test_time/</link>
      <pubDate>Wed, 07 Aug 2024 13:05:44 +0200</pubDate>
      <guid>http://localhost:3131/posts/240807_learning_at_test_time/learning_at_test_time/</guid>
      <description>Recently, a new class of models known as Test-Time Training (TTT) has emerged as a viable alternative to the longstanding transformer architecture in large language models (LLMs) [4,5]. TTT models dynamically adjust their hidden states through gradient updates, offering a novel approach to enhancing model performance and adaptability.
In this post, I document my notes about the idea of test-time-training.
Test-Time-Training The concept of test-time training first emerged in the literature as a methodology to address distributional shifts occurring at test time, where the test data originate from a distribution that differs from the source data used during model training.</description>
    </item>
    <item>
      <title>Wanna dance like a pro? A neural network can help you</title>
      <link>http://localhost:3131/posts/20191018_dance_like_pro/</link>
      <pubDate>Fri, 18 Oct 2019 21:40:07 +0200</pubDate>
      <guid>http://localhost:3131/posts/20191018_dance_like_pro/</guid>
      <description>A brief introduction to photorealistic video translation with GANs
Feng-Ting Liao, Ru-Han Wu
The original post apears here
Imagine you see your favourite star, perhaps it&amp;rsquo;s Bruno Mars, Beyonce, BTS, or TWICE, dancing amazingly in music videos. You then wonder if you could dance just as good as they do and maybe replicate their moves and reproduce what they did in the videos. Same posture, same movement and even the same tricks of yourself match at the exact timestamp to the stars in their videos.</description>
    </item>
  </channel>
</rss>
