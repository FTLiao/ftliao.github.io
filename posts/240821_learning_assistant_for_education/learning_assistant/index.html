<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Learing Assistant for Education | Feng-Ting Liao | Particles of Worlds</title>
<meta name=keywords content><meta name=description content="Learing Assistant for Education There has been much development of ML in EduTech. One notable effort as an example is Andrej&rsquo;s effort in building a start aiming to apply AI assistns to education (link to news).
In this post, I will share my notes on such a learning assistant.
I will begin by defining my understanding of learning, followed by outline the properties of a learning assistant that effectively supports a learner&rsquo;s educational journey."><meta name=author content="Feng-Ting Liao"><link rel=canonical href=https://ftliao.github.io/posts/240821_learning_assistant_for_education/learning_assistant/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.0cb1c59e6eb3ed70b18bc164f9850051b2d8799eb8d2eb0e0f108adc90650409.css integrity="sha256-DLHFnm6z7XCxi8Fk+YUAUbLYeZ640usODxCK3JBlBAk=" rel="preload stylesheet" as=style><link rel=icon href=https://ftliao.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://ftliao.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://ftliao.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://ftliao.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://ftliao.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://ftliao.github.io/posts/240821_learning_assistant_for_education/learning_assistant/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script>MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["$","$"],["\\(","\\)"]],processEscapes:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]}}</script><link rel=stylesheet type=text/css href=/hugo-cite.css><meta property="og:title" content="Learing Assistant for Education"><meta property="og:description" content="Learing Assistant for Education There has been much development of ML in EduTech. One notable effort as an example is Andrej&rsquo;s effort in building a start aiming to apply AI assistns to education (link to news).
In this post, I will share my notes on such a learning assistant.
I will begin by defining my understanding of learning, followed by outline the properties of a learning assistant that effectively supports a learner&rsquo;s educational journey."><meta property="og:type" content="article"><meta property="og:url" content="https://ftliao.github.io/posts/240821_learning_assistant_for_education/learning_assistant/"><meta property="og:image" content="https://ftliao.github.io/images/openimage.jpg"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-08-21T13:05:44+02:00"><meta property="article:modified_time" content="2024-08-21T13:05:44+02:00"><meta property="og:site_name" content="Feng-Ting Liao | Particles of Worlds"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://ftliao.github.io/images/openimage.jpg"><meta name=twitter:title content="Learing Assistant for Education"><meta name=twitter:description content="Learing Assistant for Education There has been much development of ML in EduTech. One notable effort as an example is Andrej&rsquo;s effort in building a start aiming to apply AI assistns to education (link to news).
In this post, I will share my notes on such a learning assistant.
I will begin by defining my understanding of learning, followed by outline the properties of a learning assistant that effectively supports a learner&rsquo;s educational journey."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://ftliao.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Learing Assistant for Education","item":"https://ftliao.github.io/posts/240821_learning_assistant_for_education/learning_assistant/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Learing Assistant for Education","name":"Learing Assistant for Education","description":"Learing Assistant for Education There has been much development of ML in EduTech. One notable effort as an example is Andrej\u0026rsquo;s effort in building a start aiming to apply AI assistns to education (link to news).\nIn this post, I will share my notes on such a learning assistant.\nI will begin by defining my understanding of learning, followed by outline the properties of a learning assistant that effectively supports a learner\u0026rsquo;s educational journey.","keywords":[],"articleBody":"Learing Assistant for Education There has been much development of ML in EduTech. One notable effort as an example is Andrej’s effort in building a start aiming to apply AI assistns to education (link to news).\nIn this post, I will share my notes on such a learning assistant.\nI will begin by defining my understanding of learning, followed by outline the properties of a learning assistant that effectively supports a learner’s educational journey. Subsequently, I will propose an implementation approach for an ML-based learning assistant and conclude by highlighting future research areas in machine learning that are essential for developing such an assistant in a scalable and cost-effective manner.\nTowards Utopia The necessity of an automated Learning Assistant\nI believe that learning is a process of continuous practice and reflection where one becomes aware of their limitations, pushes beyond them, and remains openess to unknowns. Ultimately, through this process, one finds fulfillment.\nTo equate learning and education for the society as a whole, there is a growing need for an automated learning assistant. For students, it offers personalized support that caters to their unique learning styles, enabling them to master subjects more effectively and confidently. Educators benefit by gaining insights into students’ progress, allowing them to focus on more nuanced and human aspects of teaching. On a broader scale, society gains by nurturing a more knowledgeable, adaptable, and skilled population, capable of innovating and solving complex problems in an increasingly complex world.\nProperties of a Learning Assistant I surmise that such an learning assistant capable of helping an inquisitive mind in mastering subjects and learning should bear the following properties:\nFacilitating Learning-to-learn\nThe assistant encourages learners to engage in active learning, helping them develop transferable strategies that can be applied across various learning contexts. This approach enables learners to use acquired knowledge in structured pathways to solve real-world problems.\nAccurate and Reliable in Response\nThe assistant should provide accurate, reliable, diverse learning materials suitable to the student’s needs and learning style, recommending resources based on their progress and adjusting as they advance.\nPersonalized Learning\nAs learners’ needs evolve, the assistant, equipped with memory of their learning history, provides tailored feedback and answers to student queries. This feedback is adjusted to their current level of knowledge and aspirations.\nMachine learning offers transformative potential in revolutionizing education by creating adaptive, personalized learning assistants that foster growth and deep understanding of knowledge. A promising realization of such a learning assistant could be built upon Large Language Models (LLMs) enhanced with Retrieval-Augmented Generation (RAG). This approach would leverage the vast knowledge and conversational capabilities of LLMs while using RAG to provide up-to-date, factual information and personalized content retrieval.\nPossible Research Directions in ML To develop a learning assistant that embodies the aforementioned properties, several critical areas in machine learning must be addressed, including, but not limited to, evaluation methods, training data quality, model architecture, reinforcement learning with human feedback (RLHF), and retrieval mechanisms as a form of slow memory. In the following section, I outline promising research directions in these areas.\nFacilitating Learning-to-Learn\nLearning how to learn empowers individuals to continually adapt to new information and skills throughout their lives. However, the role of machine learning in facilitating the acquisition of such meta-learning skills remains underexplored. Prior work by [4] incorporates high level pedagogical principles into the evaluation of LLMs across various pedagogical dimensions. One such dimension, Analogies, offers a partial assessment of an LLM’s ability to instruct learners on how to learn. The work, however, is insufficient as dimension such as abstract thinking and self-discovery could also be critical attributes for evaluating a learning assistant’s effectiveness.\nTo bridge this gap, future research could focus on developing a benchmark, created in collaboration with educators and students, that evaluates learning assistants on their ability to guide learners in discovering transferable principles across diverse tasks without explicit instruction. These tasks could be synthetic and might include challenges such as:\nLearning a systematic method for remembering long numbers that’s transferable to remembering long strings Learning to learn in breadth within a constrained timeframe when presented with abstrusive books Accurate and Reliable in Response\nA robust learning assistant should provide accurate and informative responses to learners. However, LLMs, when used as the engine of a learning assistant, are known to hallucination and fail to extract knowledge despite having seen it in pre-training. LLMs often answer questions directly, offering little to no indication of their confidence in the accuracy of their responses. This poses a risk of misleading learners, particularly those unfamiliar with the subject matter.\nAddressing these issues in future research is crucial for developing a trustworthy learning agent. Improving the accuracy of knowledge extraction in LLM responses could be achieved by incorporating a diverse set of synthetic question-answer pairs related to key knowledge points into the pre-training data, as demonstrated by [2]. Enhancing the provision of confidence levels in LLM responses might involve pre-training the model on synthetic data where knowledge points are cross-referenced and scored, or by incorporating intermediary thinking stages, similar to approaches in [5,6].\nFast adaptation for personalization\nA learning assistant should tailor materials to the learner’s aptitude and personalize the learning experience based on their learning history. This history can be encoded into the assistant’s behavior through RLHF, by prepending the history as a prefix context, or by storing it in an offline database for later RAG. However, the first two methods become uneconomical as the number of learners increases and the size of learning histories accumulates. The third method, while promising, requires rethinking as multi-modal histories, such as video and audio, become part of the assistant’s slow memory. Therefore, new innovations are needed to enable fast adaptation of personalizable learning assistants, focusing on efficiently compressing learners’ histories for personalized learning. Below are several promising research directions that could address these challenges and lead to a cost-efficient implementation of a personalizable learning assistant:\nBehavioral Adjustment through Joint Decoding:\nConventional RLHF is not scalable for adapting LLMs to personalized learning, as it involves updating both a reward model and a policy model. A potential solution is to employ joint decoding at test time, where a reward model stores information that enables personalized assistance, similar to the approach in [7]. To facilitate seamless joint decoding with diverse reward models, an RLHF framework where the hyperparameters for joint decoding during rollouts are parameterized and learned would need to be developed.\nEfficient Compression of Learning History:\nAs new modalities like video and audio become integral to personalized learning, storing uncompressed learning histories becomes impractical. One research direction could involve applying the principles of attention in LLMs as gradient updates to meta-parameters for selecting features of hidden activations, as suggested in [1]. This approach could enable the storage of billions of history tokens via meta-parameters, allowing the learning assistant to access distant parts of a learner’s history. This method could complement the use of a neural retriever as an external database, as described below.\nReformulating Retrieval with Neural Networks:\nVideo and audio are essential for constructing adaptive responses to learner queries. However, storing and indexing this history using conventional databases becomes unscalable. A promising research direction would be to reformulate retrieval using an end-to-end neural network as a compressor for storing slow memory, as explored in [3], and to enhance the update of slow memory and retrieval efficiency using meta-knowledge, similar to the method proposed in [8].\nReference [1] Learning to (Learn at Test Time): RNNs with Expressive Hidden States, Y. Sun, preprint, 2024 [2] Physics of Language Models: Part 3.1, Knowledge Storage and Extraction, Z. Allen-Zhu and Y. Li, ICML 2024 [3] End-to-End Training of Neural Retrievers for Open-Domain Question Answering, D.S. Sachan et al., ACL 2021 [4] Towards Responsible Development of Generative AI for Education: An Evaluation-Driven Approach, I. Jurenka et al., preprint, 2024 [5] Thinking Tokens for Language Modeling, D. Herel and T. Mikolov, AITP, 2023 [6] Orca: Progressive Learning from Complex Explanation Traces of GPT-4, S. Mukherjee et al., preprint, 2023 [7] Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional Reward Model, H. Deng and C. Raffel, ACL 2023 [8] Meta Knowledge for Retrieval Augmented Large Language Models, L. Mombaerts et al., KDD, 2024\n","wordCount":"1362","inLanguage":"en","image":"https://ftliao.github.io/images/openimage.jpg","datePublished":"2024-08-21T13:05:44+02:00","dateModified":"2024-08-21T13:05:44+02:00","author":{"@type":"Person","name":"Feng-Ting Liao"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://ftliao.github.io/posts/240821_learning_assistant_for_education/learning_assistant/"},"publisher":{"@type":"Organization","name":"Feng-Ting Liao | Particles of Worlds","logo":{"@type":"ImageObject","url":"https://ftliao.github.io/%3Clink%20/%20abs%20url%3E"}}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script>MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["$","$"],["\\(","\\)"]],processEscapes:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://ftliao.github.io/ accesskey=h title="Particles of Worlds (Alt + H)"><img src=https://ftliao.github.io/apple-touch-icon.png alt aria-label=logo height=35>Particles of Worlds</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://ftliao.github.io/about/ title=About><span>About</span></a></li><li><a href=https://ftliao.github.io/posts/ title=Posts><span>Posts</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://ftliao.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://ftliao.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Learing Assistant for Education</h1><div class=post-meta><span title='2024-08-21 13:05:44 +0200 CEST'>August 21, 2024</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;1362 words&nbsp;·&nbsp;Feng-Ting Liao</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#towards-utopia>Towards Utopia</a></li><li><a href=#properties-of-a-learning-assistant>Properties of a Learning Assistant</a></li><li><a href=#possible-research-directions-in-ml>Possible Research Directions in ML</a></li><li><a href=#reference>Reference</a></li></ul></nav></div></details></div><div class=post-content><h1 id=learing-assistant-for-education>Learing Assistant for Education<a hidden class=anchor aria-hidden=true href=#learing-assistant-for-education>#</a></h1><p>There has been much development of ML in EduTech. One notable effort as an example is Andrej&rsquo;s effort in building a start aiming to apply AI assistns to education (<a href=https://techcrunch.com/2024/07/16/after-tesla-and-openai-andrej-karpathys-startup-aims-to-apply-ai-assistants-to-education/>link to news</a>).</p><p>In this post, I will share my notes on such a learning assistant.</p><p>I will begin by defining my understanding of learning, followed by outline the properties of a learning assistant that effectively supports a learner&rsquo;s educational journey. Subsequently, I will propose an implementation approach for an ML-based learning assistant and conclude by highlighting future research areas in machine learning that are essential for developing such an assistant in a scalable and cost-effective manner.</p><h2 id=towards-utopia>Towards Utopia<a hidden class=anchor aria-hidden=true href=#towards-utopia>#</a></h2><p><strong>The necessity of an automated Learning Assistant</strong><br>I believe that learning is a process of continuous practice and reflection where one becomes aware of their limitations, pushes beyond them, and remains openess to unknowns. Ultimately, through this process, one finds fulfillment.</p><p>To equate learning and education for the society as a whole, there is a growing need for an automated learning assistant. For students, it offers personalized support that caters to their unique learning styles, enabling them to master subjects more effectively and confidently. Educators benefit by gaining insights into students&rsquo; progress, allowing them to focus on more nuanced and human aspects of teaching. On a broader scale, society gains by nurturing a more knowledgeable, adaptable, and skilled population, capable of innovating and solving complex problems in an increasingly complex world.</p><h2 id=properties-of-a-learning-assistant>Properties of a Learning Assistant<a hidden class=anchor aria-hidden=true href=#properties-of-a-learning-assistant>#</a></h2><p>I surmise that such an learning assistant capable of helping an inquisitive mind in mastering subjects and learning should bear the following properties:</p><ol><li><p><strong>Facilitating Learning-to-learn</strong><br>The assistant encourages learners to engage in active learning, helping them develop transferable strategies that can be applied across various learning contexts. This approach enables learners to use acquired knowledge in structured pathways to solve real-world problems.</p></li><li><p><strong>Accurate and Reliable in Response</strong><br>The assistant should provide accurate, reliable, diverse learning materials suitable to the student&rsquo;s needs and learning style, recommending resources based on their progress and adjusting as they advance.</p></li><li><p><strong>Personalized Learning</strong><br>As learners&rsquo; needs evolve, the assistant, equipped with memory of their learning history, provides tailored feedback and answers to student queries. This feedback is adjusted to their current level of knowledge and aspirations.</p></li></ol><p>Machine learning offers transformative potential in revolutionizing education by creating adaptive, personalized learning assistants that foster growth and deep understanding of knowledge. A promising realization of such a learning assistant could be built upon Large Language Models (LLMs) enhanced with Retrieval-Augmented Generation (RAG). This approach would leverage the vast knowledge and conversational capabilities of LLMs while using RAG to provide up-to-date, factual information and personalized content retrieval.</p><h2 id=possible-research-directions-in-ml>Possible Research Directions in ML<a hidden class=anchor aria-hidden=true href=#possible-research-directions-in-ml>#</a></h2><p>To develop a learning assistant that embodies the aforementioned properties, several critical areas in machine learning must be addressed, including, but not limited to, evaluation methods, training data quality, model architecture, reinforcement learning with human feedback (RLHF), and retrieval mechanisms as a form of slow memory. In the following section, I outline promising research directions in these areas.</p><p><strong>Facilitating Learning-to-Learn</strong><br>Learning how to learn empowers individuals to continually adapt to new information and skills throughout their lives. However, the role of machine learning in facilitating the acquisition of such meta-learning skills remains underexplored. Prior work by [4] incorporates high level pedagogical principles into the evaluation of LLMs across various pedagogical dimensions. One such dimension, Analogies, offers a partial assessment of an LLM&rsquo;s ability to instruct learners on how to learn. The work, however, is insufficient as dimension such as abstract thinking and self-discovery could also be critical attributes for evaluating a learning assistant&rsquo;s effectiveness.</p><p>To bridge this gap, future research could focus on developing a benchmark, created in collaboration with educators and students, that evaluates learning assistants on their ability to guide learners in discovering transferable principles across diverse tasks without explicit instruction. These tasks could be synthetic and might include challenges such as:</p><ul><li>Learning a systematic method for remembering long numbers that’s transferable to remembering long strings</li><li>Learning to learn in breadth within a constrained timeframe when presented with abstrusive books</li></ul><p><strong>Accurate and Reliable in Response</strong><br>A robust learning assistant should provide accurate and informative responses to learners. However, LLMs, when used as the engine of a learning assistant, are known to hallucination and fail to extract knowledge despite having seen it in pre-training. LLMs often answer questions directly, offering little to no indication of their confidence in the accuracy of their responses. This poses a risk of misleading learners, particularly those unfamiliar with the subject matter.</p><p>Addressing these issues in future research is crucial for developing a trustworthy learning agent. Improving the accuracy of knowledge extraction in LLM responses could be achieved by incorporating a diverse set of synthetic question-answer pairs related to key knowledge points into the pre-training data, as demonstrated by [2]. Enhancing the provision of confidence levels in LLM responses might involve pre-training the model on synthetic data where knowledge points are cross-referenced and scored, or by incorporating intermediary thinking stages, similar to approaches in [5,6].</p><p><strong>Fast adaptation for personalization</strong><br>A learning assistant should tailor materials to the learner&rsquo;s aptitude and personalize the learning experience based on their learning history. This history can be encoded into the assistant&rsquo;s behavior through RLHF, by prepending the history as a prefix context, or by storing it in an offline database for later RAG. However, the first two methods become uneconomical as the number of learners increases and the size of learning histories accumulates. The third method, while promising, requires rethinking as multi-modal histories, such as video and audio, become part of the assistant&rsquo;s slow memory. Therefore, new innovations are needed to enable fast adaptation of personalizable learning assistants, focusing on efficiently compressing learners&rsquo; histories for personalized learning. Below are several promising research directions that could address these challenges and lead to a cost-efficient implementation of a personalizable learning assistant:</p><ol><li><p><strong>Behavioral Adjustment through Joint Decoding:</strong><br>Conventional RLHF is not scalable for adapting LLMs to personalized learning, as it involves updating both a reward model and a policy model. A potential solution is to employ joint decoding at test time, where a reward model stores information that enables personalized assistance, similar to the approach in [7]. To facilitate seamless joint decoding with diverse reward models, an RLHF framework where the hyperparameters for joint decoding during rollouts are parameterized and learned would need to be developed.</p></li><li><p><strong>Efficient Compression of Learning History:</strong><br>As new modalities like video and audio become integral to personalized learning, storing uncompressed learning histories becomes impractical. One research direction could involve applying the principles of attention in LLMs as gradient updates to meta-parameters for selecting features of hidden activations, as suggested in [1]. This approach could enable the storage of billions of history tokens via meta-parameters, allowing the learning assistant to access distant parts of a learner&rsquo;s history. This method could complement the use of a neural retriever as an external database, as described below.</p></li><li><p><strong>Reformulating Retrieval with Neural Networks:</strong><br>Video and audio are essential for constructing adaptive responses to learner queries. However, storing and indexing this history using conventional databases becomes unscalable. A promising research direction would be to reformulate retrieval using an end-to-end neural network as a compressor for storing slow memory, as explored in [3], and to enhance the update of slow memory and retrieval efficiency using meta-knowledge, similar to the method proposed in [8].</p></li></ol><h2 id=reference>Reference<a hidden class=anchor aria-hidden=true href=#reference>#</a></h2><p>[1] Learning to (Learn at Test Time): RNNs with Expressive Hidden States, Y. Sun, preprint, 2024<br>[2] Physics of Language Models: Part 3.1, Knowledge Storage and Extraction, Z. Allen-Zhu and Y. Li, ICML 2024<br>[3] End-to-End Training of Neural Retrievers for Open-Domain Question Answering, D.S. Sachan et al., ACL 2021<br>[4] Towards Responsible Development of Generative AI for Education: An Evaluation-Driven Approach, I. Jurenka et al., preprint, 2024<br>[5] Thinking Tokens for Language Modeling, D. Herel and T. Mikolov, AITP, 2023<br>[6] Orca: Progressive Learning from Complex Explanation Traces of GPT-4, S. Mukherjee et al., preprint, 2023<br>[7] Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional Reward Model, H. Deng and C. Raffel, ACL 2023<br>[8] Meta Knowledge for Retrieval Augmented Large Language Models, L. Mombaerts et al., KDD, 2024</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=next href=https://ftliao.github.io/posts/240807_learning_at_test_time/learning_at_test_time/><span class=title>Next »</span><br><span>Learning at Test Time</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://ftliao.github.io/>Feng-Ting Liao | Particles of Worlds</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>